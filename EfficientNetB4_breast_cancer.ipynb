{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EfficientNetB4_breast_cancer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TWmbcq8GPXh"
      },
      "source": [
        "This is the EfficientNetB4 network for breast cancer detection using transfer learning and Adabound as an optimizer. If you want to use our codes, please cite our paper (The paper is in submission process right now) and read our license agreement file.\n",
        "\n",
        "At first, we supposed to import all the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDI9wAS_FVOz"
      },
      "source": [
        "import tensorflow as tf\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Flatten, ReLU\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam, SGD, Adadelta, adagrad\n",
        "import efficientnet.keras as enet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras_radam import RAdam\n",
        "from keras_adabound import AdaBound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVmHRxAfHLn6"
      },
      "source": [
        "To have reproducable results we can use the followin codes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD5U3S1iFpP0"
      },
      "source": [
        "''' Seed for reproducible results '''\n",
        "from keras import backend as K\n",
        "\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1863XL1HUho"
      },
      "source": [
        "Reading the images and labels from a directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z647-qtFs82"
      },
      "source": [
        "''' Read images and labels '''\n",
        "print('Reading images ...')\n",
        "\n",
        "jpgFilenamesList = glob.glob('dataset_comparison/*.jpg')\n",
        "\n",
        "sample_img = cv2.imread(jpgFilenamesList[0], cv2.IMREAD_GRAYSCALE)\n",
        "sample_shape = sample_img.shape\n",
        "dataset = np.zeros((len(jpgFilenamesList), sample_shape[0], sample_shape[1]), dtype=np.float32)\n",
        "labels = np.zeros(len(jpgFilenamesList), dtype=np.float32)\n",
        "for i in range(len(jpgFilenamesList)):\n",
        "    dataset[i] = cv2.imread(jpgFilenamesList[i], cv2.IMREAD_GRAYSCALE)\n",
        "    name = jpgFilenamesList[i].split('\\\\')[1]\n",
        "    under_split = name.split('_')[1]\n",
        "    final_split = under_split.split('.')[0]\n",
        "    labels[i] = np.float32(final_split)\n",
        "\n",
        "print(dataset.shape)\n",
        "print(labels.shape)\n",
        "print(type(dataset))\n",
        "print(type(labels))\n",
        "print(dataset[0])\n",
        "print(labels)\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print('Hereeeee')\n",
        "print(unique)\n",
        "print(counts)\n",
        "plt.imshow(dataset[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP1gFOgMHbZE"
      },
      "source": [
        "Shuffle dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EukGl7CLFx6x"
      },
      "source": [
        "''' Shuffle dataset '''\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "dataset, labels = shuffle(dataset, labels, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOuUyMUoHeHM"
      },
      "source": [
        "Normalizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buOqYXjaF0n9"
      },
      "source": [
        "''' Normalize images '''\n",
        "print('Normalization')\n",
        "dataset = dataset.astype('float32')\n",
        "print(np.max(dataset))\n",
        "print(np.min(dataset))\n",
        "dataset /= np.float(255)\n",
        "print(np.max(dataset))\n",
        "print(np.min(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hs5QsfMHjS7"
      },
      "source": [
        "Make sure that the number of channels in the image are 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zYrWKcQF2Af"
      },
      "source": [
        "''' Image channels compatible with the network '''\n",
        "# 3 Channel image:\n",
        "dataset = np.stack((dataset,) * 3, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoBq1OaH3p2"
      },
      "source": [
        "Split train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8qRYQKVF4AS"
      },
      "source": [
        "''' Split dataset '''\n",
        "x_train_1, x_test_1, y_train1, y_test1 = train_test_split(dataset, labels, test_size=0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD_AG7khH8VV"
      },
      "source": [
        "Make the labels to categorical format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPB1RLN5F5dw"
      },
      "source": [
        "''' Categorical labels '''\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train1 = to_categorical(y_train1, num_classes=len(np.unique(labels)))\n",
        "y_test1 = to_categorical(y_test1, num_classes=len(np.unique(labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRFKtG2MIBcX"
      },
      "source": [
        "Build your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoh7GpTF7PB"
      },
      "source": [
        "''' Initialization '''\n",
        "input_shape = x_train_1.shape[1:]\n",
        "print(input_shape)\n",
        "\n",
        "# loading B0 pre-trained on ImageNet without final aka fiature extractor\n",
        "model = enet.EfficientNetB4(include_top=False,\n",
        "                            pooling='avg',\n",
        "                            input_shape=(120, 120, 3),\n",
        "                            # weights='imagenet'\n",
        "                            )\n",
        "\n",
        "#  building 2 fully connected layer\n",
        "x = model.output\n",
        "\n",
        "# output layer\n",
        "predictions = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model_final = Model(inputs=model.input, outputs=predictions)\n",
        "\n",
        "model_final.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl3Ew1DoINwo"
      },
      "source": [
        "Depict model by the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrjHIfJQF--v"
      },
      "source": [
        "''' Depict model '''\n",
        "model_final.summary()\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "os.environ[\"PATH\"] += os.pathsep + 'Your path'\n",
        "plot_model(model_final, to_file='model_single.png', show_shapes=True, show_layer_names=False)\n",
        "image = cv2.imread('model_single.png')\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxjHz0FbISqg"
      },
      "source": [
        "To depict the results in each epoch, you can use following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IME8zQE3GEEI"
      },
      "source": [
        "''' Plot online results (Live results)'''\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acces = []\n",
        "        self.val_acces = []\n",
        "\n",
        "        self.fig = plt.figure()\n",
        "\n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acces.append(logs.get('acc'))\n",
        "        self.val_acces.append(logs.get('val_acc'))\n",
        "        self.i += 1\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "plot_losses = PlotLosses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bcjOgpHIbAn"
      },
      "source": [
        "Train the model with your own model metrics, losses, and optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aePTGSfuGE3X"
      },
      "source": [
        "''' Train model '''\n",
        "start_time = time.time()\n",
        "\n",
        "optm = AdaBoundOptimizer(amsbound=True,\n",
        "                         learning_rate=0.001,\n",
        "                         final_lr=0.1,\n",
        "                         beta1=0.9,\n",
        "                         beta2=0.999,\n",
        "                         gamma=1e-3,\n",
        "                         epsilon=1e-8,\n",
        "                         use_locking=False)\n",
        "# You can use your model metrics, losses, and optimizers.\n",
        "# optm = Adam()\n",
        "# optm = Adadelta()\n",
        "# optm = adagrad()\n",
        "# optm = SGD()\n",
        "\n",
        "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom=True)\n",
        "model_final.compile(loss='binary_crossentropy',\n",
        "                    optimizer=optm,\n",
        "                    metrics=['accuracy', tf.keras.metrics.AUC(), 'binary_accuracy'])\n",
        "\n",
        "mcp_save = ModelCheckpoint('EnetB0_CIFAR10_TL.h5', save_best_only=True, monitor='val_binary_accuracy')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, verbose=1, )\n",
        "\n",
        "print(\"Training....\")\n",
        "history = model_final.fit(x_train_1, y_train1,\n",
        "                          batch_size=36,\n",
        "                          epochs=30,\n",
        "                          validation_split=0.1,\n",
        "                          callbacks=[plot_losses, mcp_save],\n",
        "                          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "719cQ-q3IdN6"
      },
      "source": [
        "Plot and print the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmDWdQAHGHtr"
      },
      "source": [
        "''' Best model weight results '''\n",
        "print('\\n\\n ****** Best model weight results: ****** \\n\\n\\n')\n",
        "model_final.load_weights('EnetB0_CIFAR10_TL.h5')\n",
        "\n",
        "y_pred = model_final.predict(x_test_1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test12 = np.argmax(y_test1, axis=1)\n",
        "cm = confusion_matrix(y_test12, y_pred)\n",
        "print('Confusion Matrix')\n",
        "print(cm)\n",
        "tn, fp, fn, tp = confusion_matrix(y_test12, y_pred).ravel()\n",
        "\n",
        "precision = tp / (tp + fp)\n",
        "print(\"Precision: {}\\n\".format(precision))\n",
        "recall = tp / (tp + fn)\n",
        "print(\"Recall: {}\\n\".format(recall))\n",
        "\n",
        "print('\\nTN: {} \\n FP: {} \\n FN: {} \\n TP: {} \\n'.format(tn, fp, fn, tp))\n",
        "\n",
        "print('Accuracy')\n",
        "print(history.history['val_accuracy'])\n",
        "print(np.max(history.history['val_accuracy']))\n",
        "\n",
        "print('AUC')\n",
        "# print(history.history['val_auc'])\n",
        "print(np.max(history.history['val_auc']))\n",
        "\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "print('f1_score: {}'.format(f1_score))\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}